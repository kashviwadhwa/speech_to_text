{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9139952,
          "sourceType": "datasetVersion",
          "datasetId": 5520015
        },
        {
          "sourceId": 9140275,
          "sourceType": "datasetVersion",
          "datasetId": 5520275
        }
      ],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook150dc5ea4c",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'fileurl:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5520015%2F9139952%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240901%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240901T151608Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D40dc225863cacf26d17827750afa30013536a887695cc2b615fa0780bb9eec291edfc159906932cb5ffa13995e66acb55159973b478a2468589ab8f06518c7e3a27d9a19f8c556bf3b87b2a6048e484a7a41cad8958880140e394f9578cb73a47ea43b6ea209677a375192bde869b23e13392000fcf86e990f6b232d39ef8ddfe0890e97e45839f3e12ef029217394086534f1706af9888dcf03c4140481394b3b065e79802f7b87e21d7a996fa16578472e7e1aa1a76de54d468f8844a442b04d00ae9de375fa83d7d375b8e005297bece73da2d85fa16aa86d1c36783873b8014b6af04da995d14966c38f015bda8846903318963f1a6518915802e771c552,hindiaudio:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5520275%2F9140275%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240901%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240901T151608Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D565b66893b7827df42b58b6c28c257358f8d50a8eaf154cbda050f7ee3b02e931df7d45db8786c60301195067e8d4c8c9829f3ddcb3dbc07ff0dbbc3488ba0cca0649ac1b031d3fc6a914cbb8a543fa17099d92fe7aa9a3e4efe3b3862a3c798cc343de36755fb26775399a30281c17e06adc2992bd99aa438207ed01007fc023e40dbf3fa16e3c65827d48e7e42c447b23ca832215c4dfc2a48c3ab3275e99d50a3b79db3dd6c0a025ca5eab3878dc9d68d0fca5140a088cb3bde6dd15274f49dc809469f76c6fc49d2ffc50756eafd459ac5a5e9b2e527598b0d09dd732f3d1864d6c84f3ac7073f37621064a4f64f7b0d2c5a40c0896c4f4e942b36fcbae8'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "5pc0KCawO3Ur"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "NAz1XzGVO3Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cog"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-09T09:20:39.590716Z",
          "iopub.execute_input": "2024-08-09T09:20:39.591069Z",
          "iopub.status.idle": "2024-08-09T09:20:56.685168Z",
          "shell.execute_reply.started": "2024-08-09T09:20:39.591042Z",
          "shell.execute_reply": "2024-08-09T09:20:56.683981Z"
        },
        "trusted": true,
        "id": "rIu-LmyxO3Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyannote.audio"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-09T09:21:40.522314Z",
          "iopub.execute_input": "2024-08-09T09:21:40.523187Z",
          "iopub.status.idle": "2024-08-09T09:24:18.517535Z",
          "shell.execute_reply.started": "2024-08-09T09:21:40.523148Z",
          "shell.execute_reply": "2024-08-09T09:24:18.516458Z"
        },
        "trusted": true,
        "id": "PzYXizbfO3Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faster-whisper"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-09T09:25:21.650128Z",
          "iopub.execute_input": "2024-08-09T09:25:21.650437Z",
          "iopub.status.idle": "2024-08-09T09:25:34.96738Z",
          "shell.execute_reply.started": "2024-08-09T09:25:21.650407Z",
          "shell.execute_reply": "2024-08-09T09:25:34.966124Z"
        },
        "trusted": true,
        "id": "ub5GzaUiO3Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-09T09:28:46.622456Z",
          "iopub.execute_input": "2024-08-09T09:28:46.623378Z",
          "iopub.status.idle": "2024-08-09T09:28:59.944836Z",
          "shell.execute_reply.started": "2024-08-09T09:28:46.62334Z",
          "shell.execute_reply": "2024-08-09T09:28:59.943869Z"
        },
        "trusted": true,
        "id": "xvQ6z1TyO3Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List\n",
        "import base64\n",
        "import datetime\n",
        "import subprocess\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import torch\n",
        "import re\n",
        "\n",
        "from cog import BasePredictor, BaseModel, Input, Path\n",
        "from faster_whisper import WhisperModel\n",
        "from pyannote.audio import Pipeline\n",
        "import torchaudio\n",
        "\n",
        "\n",
        "class Output(BaseModel):\n",
        "    segments: list\n",
        "    language: str = None\n",
        "    num_speakers: int = None\n",
        "\n",
        "\n",
        "class Predictor(BasePredictor):\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n",
        "        model_name = \"medium\"\n",
        "        self.model = WhisperModel(\n",
        "            model_name,\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            compute_type=\"float32\",\n",
        "        )\n",
        "\n",
        "        # Replace with the provided token\n",
        "        self.diarization_model = Pipeline.from_pretrained(\n",
        "            \"pyannote/speaker-diarization-3.1\",\n",
        "            use_auth_token=\"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\",\n",
        "        )\n",
        "\n",
        "        if self.diarization_model is None:\n",
        "            raise RuntimeError(\"Failed to load the diarization model. Please check your token or model access permissions.\")\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        file_string: str = Input(\n",
        "            description=\"Either provide: Base64 encoded audio file\", default=None\n",
        "        ),\n",
        "        file_path: Path = Input(description=\"Or an audio file path\", default=None),\n",
        "        group_segments: bool = Input(\n",
        "            description=\"Group segments of same speaker shorter apart than 2 seconds\",\n",
        "            default=True,\n",
        "        ),\n",
        "        transcript_output_format: str = Input(\n",
        "            description=\"Specify the format of the transcript output: individual words with timestamps, full text of segments, or a combination of both.\",\n",
        "            default=\"both\",\n",
        "            choices=[\"words_only\", \"segments_only\", \"both\"],\n",
        "        ),\n",
        "        num_speakers: int = Input(\n",
        "            description=\"Number of speakers, leave empty to autodetect.\",\n",
        "            ge=1,\n",
        "            le=50,\n",
        "            default=None,\n",
        "        ),\n",
        "        translate: bool = Input(\n",
        "            description=\"Translate the speech into English.\",\n",
        "            default=False,\n",
        "        ),\n",
        "        language: str = Input(\n",
        "            description=\"Language of the spoken words as a language code like 'en'. Leave empty to auto detect language.\",\n",
        "            default=None,\n",
        "        ),\n",
        "        prompt: str = Input(\n",
        "            description=\"Vocabulary: provide names, acronyms and loanwords in a list. Use punctuation for best accuracy.\",\n",
        "            default=None,\n",
        "        ),\n",
        "        offset_seconds: int = Input(\n",
        "            description=\"Offset in seconds, used for chunked inputs\", default=0, ge=0\n",
        "        ),\n",
        "    ) -> Output:\n",
        "        \"\"\"Run a single prediction on the model\"\"\"\n",
        "        try:\n",
        "            # Generate a temporary filename\n",
        "            temp_wav_filename = f\"temp-{time.time_ns()}.wav\"\n",
        "\n",
        "            if file_path is not None:\n",
        "                # Convert the provided file path to WAV format\n",
        "                subprocess.run(\n",
        "                    [\n",
        "                        \"ffmpeg\",\n",
        "                        \"-i\",\n",
        "                        file_path,\n",
        "                        \"-ar\",\n",
        "                        \"16000\",\n",
        "                        \"-ac\",\n",
        "                        \"1\",\n",
        "                        \"-c:a\",\n",
        "                        \"pcm_s16le\",\n",
        "                        temp_wav_filename,\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "            elif file_string is not None:\n",
        "                audio_data = base64.b64decode(\n",
        "                    file_string.split(\",\")[1] if \",\" in file_string else file_string\n",
        "                )\n",
        "                temp_audio_filename = f\"temp-{time.time_ns()}.audio\"\n",
        "                with open(temp_audio_filename, \"wb\") as f:\n",
        "                    f.write(audio_data)\n",
        "\n",
        "                subprocess.run(\n",
        "                    [\n",
        "                        \"ffmpeg\",\n",
        "                        \"-i\",\n",
        "                        temp_audio_filename,\n",
        "                        \"-ar\",\n",
        "                        \"16000\",\n",
        "                        \"-ac\",\n",
        "                        \"1\",\n",
        "                        \"-c:a\",\n",
        "                        \"pcm_s16le\",\n",
        "                        temp_wav_filename,\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                if os.path.exists(temp_audio_filename):\n",
        "                    os.remove(temp_audio_filename)\n",
        "\n",
        "            segments, detected_num_speakers, detected_language = self.speech_to_text(\n",
        "                temp_wav_filename,\n",
        "                num_speakers,\n",
        "                prompt,\n",
        "                offset_seconds,\n",
        "                group_segments,\n",
        "                language,\n",
        "                word_timestamps=True,\n",
        "                transcript_output_format=transcript_output_format,\n",
        "                translate=translate,\n",
        "            )\n",
        "\n",
        "            print(f\"done with inference\")\n",
        "            # Return the results as a JSON object\n",
        "            return Output(\n",
        "                segments=segments,\n",
        "                language=detected_language,\n",
        "                num_speakers=detected_num_speakers,\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Error Running inference with local model\", e)\n",
        "\n",
        "        finally:\n",
        "            # Clean up\n",
        "            if os.path.exists(temp_wav_filename):\n",
        "                os.remove(temp_wav_filename)\n",
        "\n",
        "    def convert_time(self, secs, offset_seconds=0):\n",
        "        return datetime.timedelta(seconds=(round(secs) + offset_seconds))\n",
        "\n",
        "    def speech_to_text(\n",
        "        self,\n",
        "        audio_file_wav,\n",
        "        num_speakers=None,\n",
        "        prompt=\"\",\n",
        "        offset_seconds=0,\n",
        "        group_segments=True,\n",
        "        language=None,\n",
        "        word_timestamps=True,\n",
        "        transcript_output_format=\"both\",\n",
        "        translate=False,\n",
        "    ):\n",
        "        time_start = time.time()\n",
        "\n",
        "        # Transcribe audio\n",
        "        print(\"Starting transcribing\")\n",
        "        options = dict(\n",
        "            vad_filter=True,\n",
        "            vad_parameters=dict(min_silence_duration_ms=1000),\n",
        "            initial_prompt=prompt,\n",
        "            word_timestamps=word_timestamps,\n",
        "            language=language,\n",
        "            task=\"translate\" if translate else \"transcribe\",\n",
        "            hotwords=prompt\n",
        "        )\n",
        "        segments, transcript_info = self.model.transcribe(audio_file_wav, **options)\n",
        "        segments = list(segments)\n",
        "        segments = [\n",
        "            {\n",
        "                \"avg_logprob\": s.avg_logprob,\n",
        "                \"start\": float(s.start + offset_seconds),\n",
        "                \"end\": float(s.end + offset_seconds),\n",
        "                \"text\": s.text,\n",
        "                \"words\": [\n",
        "                    {\n",
        "                        \"start\": float(w.start + offset_seconds),\n",
        "                        \"end\": float(w.end + offset_seconds),\n",
        "                        \"word\": w.word,\n",
        "                        \"probability\": w.probability,\n",
        "                    }\n",
        "                    for w in s.words\n",
        "                ],\n",
        "            }\n",
        "            for s in segments\n",
        "        ]\n",
        "\n",
        "        time_transcribing_end = time.time()\n",
        "        print(\n",
        "            f\"Finished with transcribing, took {time_transcribing_end - time_start:.5} seconds\"\n",
        "        )\n",
        "\n",
        "        print(\"Starting diarization\")\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_wav)\n",
        "        diarization = self.diarization_model(\n",
        "            {\"waveform\": waveform, \"sample_rate\": sample_rate},\n",
        "            num_speakers=num_speakers,\n",
        "        )\n",
        "\n",
        "        time_diarization_end = time.time()\n",
        "        print(\n",
        "            f\"Finished with diarization, took {time_diarization_end - time_transcribing_end:.5} seconds\"\n",
        "        )\n",
        "\n",
        "        print(\"Starting merging\")\n",
        "\n",
        "        # Initialize variables to keep track of the current position in both lists\n",
        "        margin = 0.1  # 0.1 seconds margin\n",
        "\n",
        "        # Initialize an empty list to hold the final segments with speaker info\n",
        "        final_segments = []\n",
        "\n",
        "        diarization_list = list(diarization.itertracks(yield_label=True))\n",
        "        unique_speakers = {\n",
        "            speaker for _, _, speaker in diarization.itertracks(yield_label=True)\n",
        "        }\n",
        "        detected_num_speakers = len(unique_speakers)\n",
        "\n",
        "        speaker_idx = 0\n",
        "        n_speakers = len(diarization_list)\n",
        "\n",
        "        # Iterate over each segment\n",
        "        for segment in segments:\n",
        "            segment_start = segment[\"start\"] + offset_seconds\n",
        "            segment_end = segment[\"end\"] + offset_seconds\n",
        "            segment_text = []\n",
        "            segment_words = []\n",
        "\n",
        "            # Iterate over each word in the segment\n",
        "            for word in segment[\"words\"]:\n",
        "                word_start = word[\"start\"] + offset_seconds - margin\n",
        "                word_end = word[\"end\"] + offset_seconds + margin\n",
        "\n",
        "                while speaker_idx < n_speakers:\n",
        "                    turn, _, speaker = diarization_list[speaker_idx]\n",
        "\n",
        "                    if turn.start <= word_end and turn.end >= word_start:\n",
        "                        # Add word without modifications\n",
        "                        segment_text.append(word[\"word\"])\n",
        "\n",
        "                        # Strip here for individual word storage\n",
        "                        word[\"word\"] = word[\"word\"].strip()\n",
        "                        segment_words.append(word)\n",
        "\n",
        "                        if turn.end <= word_end:\n",
        "                            speaker_idx += 1\n",
        "\n",
        "                        break\n",
        "                    elif turn.end < word_start:\n",
        "                        speaker_idx += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            if segment_text:\n",
        "                combined_text = \"\".join(segment_text)\n",
        "                cleaned_text = re.sub(\"  \", \" \", combined_text).strip()\n",
        "                new_segment = {\n",
        "                    \"avg_logprob\": segment[\"avg_logprob\"],\n",
        "                    \"start\": segment_start - offset_seconds,\n",
        "                    \"end\": segment_end - offset_seconds,\n",
        "                    \"speaker\": speaker,\n",
        "                    \"text\": cleaned_text,\n",
        "                    \"words\": segment_words,\n",
        "                }\n",
        "                final_segments.append(new_segment)\n",
        "\n",
        "        time_merging_end = time.time()\n",
        "        print(\n",
        "            f\"Finished with merging, took {time_merging_end - time_diarization_end:.5} seconds\"\n",
        "        )\n",
        "\n",
        "        print(\"Starting cleaning\")\n",
        "        segments = final_segments\n",
        "        # Make output\n",
        "        output = []  # Initialize an empty list for the output\n",
        "\n",
        "        # Initialize the first group with the first segment\n",
        "        current_group = segments[0]\n",
        "        output.append(current_group)\n",
        "\n",
        "        for segment in segments[1:]:\n",
        "            # If group_segments is True and the speaker is the same as the last one,\n",
        "            # merge segments that are close to each other (less than 2 seconds apart)\n",
        "            if (\n",
        "                group_segments\n",
        "                and segment[\"speaker\"] == current_group[\"speaker\"]\n",
        "                and segment[\"start\"] - current_group[\"end\"] < 2\n",
        "            ):\n",
        "                # Extend the end time and append the new text and words to the current group\n",
        "                current_group[\"end\"] = segment[\"end\"]\n",
        "                current_group[\"text\"] += \" \" + segment[\"text\"]\n",
        "                current_group[\"words\"].extend(segment[\"words\"])\n",
        "            else:\n",
        "                # Otherwise, start a new group\n",
        "                current_group = segment\n",
        "                output.append(current_group)\n",
        "\n",
        "        segments = output\n",
        "\n",
        "        time_cleaning_end = time.time()\n",
        "        print(\n",
        "            f\"Finished with cleaning, took {time_cleaning_end - time_merging_end:.5} seconds\"\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Finished with all, total time: {time_cleaning_end - time_start:.5} seconds\"\n",
        "        )\n",
        "\n",
        "        return segments, detected_num_speakers, transcript_info.language\n",
        "\n",
        "# URL\n",
        "file_path = \"/kaggle/input/hindiaudio/Audio 2.2.mp3\"\n",
        "\n",
        "# Instantiate the Predictor class\n",
        "predictor = Predictor()\n",
        "\n",
        "# Setup the models\n",
        "predictor.setup()\n",
        "\n",
        "# Call the predict method with the file_path\n",
        "result = predictor.predict(\n",
        "    file_path=file_path,         # Passing the file path\n",
        "    group_segments=True,        # Additional settings as needed\n",
        "    transcript_output_format=\"both\",\n",
        "    num_speakers=None,\n",
        "    translate=False,\n",
        "    language=\"hi\",              # Set the language to Hindi\n",
        "    prompt=\"नमस्ते, भारत, हिंदी\",\n",
        "    offset_seconds=0\n",
        ")\n",
        "\n",
        "\n",
        "# Print the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-09T11:15:47.948239Z",
          "iopub.execute_input": "2024-08-09T11:15:47.948593Z",
          "iopub.status.idle": "2024-08-09T11:17:22.898214Z",
          "shell.execute_reply.started": "2024-08-09T11:15:47.948567Z",
          "shell.execute_reply": "2024-08-09T11:17:22.897282Z"
        },
        "trusted": true,
        "id": "7zwv3RqIO3Uu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}